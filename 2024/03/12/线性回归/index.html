
  <!DOCTYPE html>
  <html lang="zh-CN"  data-theme="dark" >
  <head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script src="https://www.googletagmanager.com/gtag/js?id=true"></script>
  <script data-pjax>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'true');
  </script>
  <!-- End Google Analytics -->


  

  
  <script async src="https://hm.baidu.com/hm.js?true"></script>
  <script data-pjax>
    var _hmt = _hmt || [];
    _hmt.push(["_trackPageview", location.pathname]);
  </script>


  
  <script>window.icon_font = '4552607_tq6stt6tcg';window.clipboard_tips = {"success":"复制成功(*^▽^*)","fail":"复制失败 (ﾟ⊿ﾟ)ﾂ","copyright":{"enable":false,"count":50,"content":"本文版权：本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！"}};window.outdate = {"enable":true,"daysAgo":180,"message":"本文最后更新于 {time}，请注意文中内容可能已经发生变化。"};</script>
  
  
  <title>
    线性回归 |
    
    yyjccc的博客
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin><link rel="preload" as="style" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CUbuntu%20Mono:400,400italic,700,700italic&display=swap"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CUbuntu%20Mono:400,400italic,700,700italic&display=swap" media="print" onload="this.media&#x3D;&#39;all&#39;">
  
    <link rel="preload" href="//at.alicdn.com/t/c/font_4552607_tq6stt6tcg.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  
  
    
<link rel="stylesheet" href="/css/loader.css">

  
  <meta name="description" content="线性回归（Linear Regression）机器学习中的两大类问题：回归问题和分类问题  回归问题就是进行预测，如股票、房价预测分类问题就是将多个事物进行分类  线性回归是一种用于预测连续数值输出的监督学习算法，它通过建立一个线性方程来描述输入变量与输出变量之间的关系。该算法的目标是使预测值与真实值之间的差异最小化。  概述线性回归是通过一个或多个自变量与因变量之间进行建模的回归分析，其特点为一">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="https://yyjccc.github.io/2024/03/12/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="yyjccc的博客">
<meta property="og:description" content="线性回归（Linear Regression）机器学习中的两大类问题：回归问题和分类问题  回归问题就是进行预测，如股票、房价预测分类问题就是将多个事物进行分类  线性回归是一种用于预测连续数值输出的监督学习算法，它通过建立一个线性方程来描述输入变量与输出变量之间的关系。该算法的目标是使预测值与真实值之间的差异最小化。  概述线性回归是通过一个或多个自变量与因变量之间进行建模的回归分析，其特点为一">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/1.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/2.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/3.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/4.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/5.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/6.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/7.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/8.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/9.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/10.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/11.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/12.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/13.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/14.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/15.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/16.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/17.svg">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/18.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/19.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/20.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/21.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/22.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/23.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/24.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/25.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/26.png">
<meta property="og:image" content="https://yyjccc.github.io/img/3-13/27.png">
<meta property="article:published_time" content="2024-03-12T15:45:59.877Z">
<meta property="article:modified_time" content="2024-03-12T15:59:32.457Z">
<meta property="article:author" content="Yyjccc">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="线性回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yyjccc.github.io/img/3-13/1.svg">
  
    <link rel="alternate" href="/atom.xml" title="yyjccc的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  <link rel="preload" href="https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  
  
    
      
<link rel="stylesheet" href="https://npm.webcache.cn/gitalk/dist/gitalk.css">

    
  
  
<script src="https://npm.webcache.cn/pace-js@1.2.4/pace.min.js"></script>

  
    
<link rel="stylesheet" href="https://npm.webcache.cn/@reimujs/aos@0.0.1/dist/aos.css">

  
<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id='loader'>
    <div class="loading-left-bg loading-bg"></div>
    <div class="loading-right-bg loading-bg"></div>
    <div class="spinner-box">
      <div class="loading-taichi">
        <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="https://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
          <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff5252" />
          <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z 
         M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95z" fill="#ff5252" />
        </svg>
      </div>
      <div class="loading-word">少女祈祷中...</div>
    </div>
  </div>
  </div>
  <script>
    var time = null;
    var startLoading = () => {
      time = Date.now();
      document.getElementById('loader').classList.remove("loading");
    }
    var endLoading = () => {
      if (!time) {
        document.body.style.overflow = 'auto';
        document.getElementById('loader').classList.add("loading");
      } else {
        if (Date.now() - time > 500) {
          time = null;
          document.body.style.overflow = 'auto';
          document.getElementById('loader').classList.add("loading");
        } else {
          setTimeout(endLoading, 500 - (Date.now() - time));
          time = null;
        }
      }
    }
    window.addEventListener('DOMContentLoaded', endLoading);
    document.getElementById('loader').addEventListener('click', endLoading);
  </script>

<div id="copy-tooltip" style="pointer-events: none; opacity: 0; transition: all 0.2s ease; position: fixed;top: 50%;left: 50%;z-index: 999;transform: translate(-50%, -50%);color: white;background: rgba(0, 0, 0, 0.5);padding: 10px 15px;border-radius: 10px;">
</div>


    <div id="container">
      <div id="wrap">
        <div id="header-nav">
  <nav id="main-nav">
    
      <span class="main-nav-link-wrap">
        <div class="main-nav-icon icon-taichi"></div>
        <a class="main-nav-link" href="/">首页</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class="main-nav-icon icon-taichi"></div>
        <a class="main-nav-link" href="/archives">归档</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class="main-nav-icon icon-taichi"></div>
        <a class="main-nav-link" href="/about">关于</a>
      </span>
    
      <span class="main-nav-link-wrap">
        <div class="main-nav-icon icon-taichi"></div>
        <a class="main-nav-link" href="/friend">友链</a>
      </span>
    
    <a id="main-nav-toggle" class="nav-icon"></a>
  </nav>
  <nav id="sub-nav">
    
      <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a>
    
    
    
      <a id="nav-search-btn" class="nav-icon popup-trigger" title="搜索"></a>
    
  </nav>
</div>
<header id="header">
  
    
      <img fetchpriority="high" src="/images/banner.webp" alt="线性回归">
    
  
  <div id="header-outer">
    <div id="header-title">
      
        
        
          <a href="/" id="logo">
            <h1 data-aos="slide-up">线性回归</h1>
          </a>
        
      
      
        
        <h2 id="subtitle-wrap" data-aos="slide-down">
          
        </h2>
      
    </div>
  </div>
</header>

        <div id="content">
          
          <section id="main"><article id="post-线性回归" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner" data-aos="fade-up">
    <div class="article-meta">
      <div class="article-date">
  <a href="/2024/03/12/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" class="article-date-link" data-aos="zoom-in">
    <time datetime="2024-03-12T15:45:59.877Z" itemprop="datePublished">2024-03-12</time>
    <time style="display: none;" id="post-update-time">2024-03-12</time>
  </a>
</div>

      

    </div>
    <div class="hr-line"></div>
    

    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote id="outdate-blockquote" style="display: none;"><p></p></blockquote>
      
      
        <h1 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h1><p>机器学习中的两大类问题：回归问题和分类问题</p>
<blockquote>
<p>回归问题就是进行预测，如股票、房价预测<br>分类问题就是将多个事物进行分类</p>
</blockquote>
<p>线性回归是一种用于预测连续数值输出的监督学习算法，它通过建立一个线性方程来描述输入变量与输出变量之间的关系。该算法的目标是使预测值与真实值之间的差异最小化。</p>
<p><a name="u4Zwx"></a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>线性回归是通过<strong>一个或多个自变量与因变量</strong>之间进行建模的回归分析，其特点为一个或多个称为回归系数的模型参数的线性组合。样本点为历史数据，<strong>回归曲线要能最贴切的模拟样本点的趋势，将误差降到最小</strong>。</p>
<p><a name="Inoua"></a></p>
<h3 id="线性回归方程"><a href="#线性回归方程" class="headerlink" title="线性回归方程"></a>线性回归方程</h3><p> 线形回归方程，就是有 <strong>n</strong> 个特征，然后每个特征 **Xi **都有相应的系数 **Wi **，并且在所有特征值为0的情况下，目标值有一个默认值 **W0 **，因此：<br><img src="/img/3-13/1.svg"><br><strong>整合后的公式为</strong>：</p>
<img src="/img/3-13/2.svg">

<p><a name="ETAVk"></a></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><a name="NmYVp"></a></p>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p> 损失函数是一个贯穿整个机器学习的一个重要概念，大部分机器学习算法都有误差，我们需要通过显性的公式来描述这个误差，并将这个误差优化到最小值。<br><strong>误差项</strong>：<br><img src="/img/3-13/3.svg"><br>用来表示真实值和预测值之间的差异</p>
<p>假设现在<strong>真实的值</strong>为 <strong>y</strong>，<strong>预测的值</strong>为 <strong>h</strong> ,损失函数为：<br><img src="/img/3-13/4.svg"></p>
<blockquote>
<p>其实机器学习在算法大思想下，关键是如何迭代调整参数能使得误差变小，<br>一般情况下算法最后推出的数学公式也是无法直接计算求解</p>
</blockquote>
<p>上面损失函数也就是所有误差和的平方。损失函数值越小，说明误差越小，这个损失函数也称<strong>最小二乘法</strong>。</p>
<p><a name="lXo7T"></a></p>
<h3 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h3><p><strong>前提假设：</strong>误差<img src="/img/3-13/5.svg">是独立并具有相同的分布，并且服从均值为0，方差为<img src="/img/3-13/6.svg">的高斯分布</p>
<p>独立同分布（independent and identically distributed，i.i.d.）：<br>在概率统计理论中，指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。</p>
<p>高斯分布：<br>即正态分布，概率密度函数如下如下：<br><img src="/img/3-13/7.svg"><br>这里就是取μ为0，即<br><img src="/img/3-13/8.svg"><br>将误差项计算公式,带入概率密度公式得到，预测准确的概率：<br><img src="/img/3-13/9.svg"></p>
<p>由于是多个样本，每个样本都是上面概率公式，这里就非常适合使用极大似然法，目标是求取一组W参数向量，使得概率取最大值<br>即求函数极值点<br>极大似然就不多说了，复习概率论就行</p>
<p>似然函数：<br><img src="/img/3-13/10.svg"><br>对似然函数取对数，将乘法转为加法</p>
<p>将上面得到式子化简得到：<br><img src="/img/3-13/11.svg"></p>
<p>求最大值转化为求下面的最小值<br><img src="/img/3-13/12.svg"></p>
<p><a name="n13p4"></a></p>
<h3 id="特殊解"><a href="#特殊解" class="headerlink" title="特殊解"></a>特殊解</h3><p>目标函数：<br><img src="/img/3-13/13.svg"></p>
<p>化简：其中<br><img src="/img/3-13/14.png"></p>
<p>一般都不能直接算，因为这里只是特殊解，要求<img src="/img/3-13/15.svg">要有逆元</p>
<p><a name="Q5JgW"></a></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p><img src="/img/3-13/16.png"><br> </p>
<blockquote>
<p>上面最小二乘法无法直接求解，但可以使用梯度下降算法逼近最小值</p>
</blockquote>
<p>梯度就是函数对它的各个自变量求偏导后，由偏导数组成的一个向量。</p>
<p>数学推导：<a target="_blank" rel="noopener" href="https://dsfftp.readthedocs.io/zh-cn/latest/Linear-Regression/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86.html">梯度下降法的数学原理 — 数据科学：从基础到实战 version 3a0c599</a><br>大概公式就是		</p>
<img src="/img/3-13/17.svg">

<p>λ表示步长或者说是学习系数，由我们定义，更加梯度下降原理，每次向着导数的反方向移动，就会逼近最小点</p>
<ul>
<li>λ取太小，会陷入局部极值点</li>
<li>λ取太大，学习成本非常高</li>
</ul>
<p><a name="THeBg"></a></p>
<h3 id="批量梯度下降（BGD）"><a href="#批量梯度下降（BGD）" class="headerlink" title="批量梯度下降（BGD）"></a>批量梯度下降（BGD）</h3><p>批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。<br>我们的目的是要<strong>误差函数尽可能的小</strong>，即求解weights使误差函数尽可能小。首先，我们随机初始化weigths，然后<strong>不断反复的更新weights使得误差函数减小，</strong>直到满足要求时停止。这里更新算法我们选择梯度下降算法，利用初始化的weights并且反复更新weights：<br><img src="/img/3-13/18.png"><br>这里代表学习率，表示<strong>每次向着J最陡峭的方向迈步的大小</strong>。为了更新weights，<strong>我们需要求出函数J的偏导数。首先当我们只有一个数据点（x,y）的时候，J的偏导数是：</strong><br><img src="/img/3-13/19.png"><br>则对<strong>所有数据点，</strong>上述损失函数的偏导（<strong>累和</strong>）为：<br><img src="/img/3-13/20.png"><br>再最小化损失函数的过程中，<strong>需要不断反复的更新weights使得误差函数减小</strong>，更新过程如下：<br><img src="/img/3-13/21.png"><br>那么好了，<strong>每次参数更新的伪代码</strong>如下：<br><img src="/img/3-13/22.png"><br>由上图更新公式我们就可以看到，<strong>我们每一次的参数更新都用到了所有的训练数据</strong>（比如有m个，就用到了m个），如果训练数据非常多的话，<strong>是非常耗时的。</strong><br><strong>下面给出批梯度下降的收敛图：</strong></p>
<p><img src="/img/3-13/23.png"><br>从图中，我们可以得到BGD迭代的次数相对较少。<br>代码实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batchGradientDescent</span>(<span class="params">x, y, theta, alpha, m, maxIteration</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(maxIteration):</span><br><span class="line">        hypothesis = np.dot(x, theta)</span><br><span class="line">        loss = hypothesis - y</span><br><span class="line">        gradient = np.dot(x.transpose(), loss) / m</span><br><span class="line">        theta = theta - alpha * gradient              <span class="comment"># 对所有样本求和</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></tbody></table></figure>
<p><a name="abTdI"></a></p>
<h3 id="随机梯度下降法（SGD）"><a href="#随机梯度下降法（SGD）" class="headerlink" title="随机梯度下降法（SGD）"></a>随机梯度下降法（SGD）</h3><p>由于批梯度下降每跟新一个参数的时候，要用到所有的样本数，所以训练速度会随着样本数量的增加而变得非常缓慢。随机梯度下降正是为了解决这个办法而提出的。它是利用每个样本的损失函数对θ求偏导得到对应的梯度，来更新θ：<br><img src="/img/3-13/24.png"><br>更新过程如下：<br><img src="/img/3-13/25.png"><br>随机梯度下降是通过每个样本来迭代更新一次，对比上面的批量梯度下降，迭代一次需要用到所有训练样本（<strong>往往如今真实问题训练数据都是非常巨大</strong>），一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。<br><strong>但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</strong><br><strong>随机梯度下降收敛图如下：</strong><br><img src="/img/3-13/26.png"><br>我们可以从图中看出SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。<strong>但是大体上是往着最优值方向移动。</strong><br><strong>代码实现：</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">StochasticGradientDescent</span>(<span class="params">x, y, theta, alpha, m, maxIteration</span>):</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        data.append(i)</span><br><span class="line">    <span class="comment"># 这里随便挑选一个进行更新点进行即可（不用想BGD一样全部考虑）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(maxIteration):</span><br><span class="line">        hypothesis = np.dot(x, theta)</span><br><span class="line">        loss = hypothesis - y  <span class="comment"># 这里还是有十个样本</span></span><br><span class="line">        index = random.sample(data, <span class="number">1</span>)[<span class="number">0</span>]  <span class="comment"># 随机抽取一个样本，得到它的下标</span></span><br><span class="line">        gradient = loss[index] * x[index]  <span class="comment"># 只取一个点进行更新计算</span></span><br><span class="line">        theta = theta - alpha * gradient.T</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></tbody></table></figure>


<p><a name="Ly3Fu"></a></p>
<h3 id="小批量梯度下降法（MBGD）"><a href="#小批量梯度下降法（MBGD）" class="headerlink" title="小批量梯度下降法（MBGD）"></a>小批量梯度下降法（MBGD）</h3><p>我们从上面两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？既<strong>算法的训练过程比较快，而且也要保证最终参数训练的准确率，</strong>而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。<br>我们假设每次更新参数的时候用到的样本数为10个（<strong>不同的任务完全不同，这里举一个例子而已</strong>）<br>更新伪代码如下：<br><img src="/img/3-13/27.png"></p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2359619">https://cloud.tencent.com/developer/article/2359619</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zongfa/p/9293887.html">https://www.cnblogs.com/zongfa/p/9293887.html</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      
      
      
      
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item" data-aos="zoom-in"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item" data-aos="zoom-in"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="article-tag-list-item" data-aos="zoom-in"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>


    </footer>
  </div>
  
    
  <nav id="article-nav" data-aos="fade-up">
    
      <div class="article-nav-link-wrap article-nav-link-left">
        
          
          
            <img data-src="https://yyjccc.github.io/images/cover/5.jpg" data-sizes="auto" alt="内存马Tomcat回显" class="lazyload">
          
        
        <a href="/2024/03/16/%E5%86%85%E5%AD%98%E9%A9%ACTomcat%E5%9B%9E%E6%98%BE/"></a>
        <div class="article-nav-caption">前一篇</div>
        <h3 class="article-nav-title">
          
            内存马Tomcat回显
          
        </h3>
      </div>
    
    
    <div class="article-nav-link-wrap article-nav-link-right">
      
        
        
          <img data-src="https://yyjccc.github.io/images/cover/1.jpg" data-sizes="auto" alt="Spring内存马" class="lazyload">
        
      
      <a href="/2024/03/12/Spring%E5%86%85%E5%AD%98%E9%A9%AC/"></a>
      <div class="article-nav-caption">后一篇</div>
      <h3 class="article-nav-title">
        
          Spring内存马
        
      </h3>
    </div>
    
  </nav>


  
</article>




  <div id="comments" class="gtcomment"></div>



</section>
          
            <aside id="sidebar">
  <div class="sidebar-wrapper wrap-sticky">
    <div class="sidebar-wrap" data-aos="fade-up">
      
        <div class="sidebar-toc-sidebar"><div class="sidebar-toc">
  <h3 class="toc-title">文章目录</h3>
  <div class="sidebar-toc-wrapper toc-div-class" >
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear-Regression%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">线性回归（Linear Regression）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">线性回归方程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.2.2.</span> <span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E6%AE%8A%E8%A7%A3"><span class="toc-number">1.2.3.</span> <span class="toc-text">特殊解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88BGD%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">批量梯度下降（BGD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88SGD%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">随机梯度下降法（SGD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88MBGD%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">小批量梯度下降法（MBGD）</span></a></li></ol></li></ol></li></ol>
      
  </div>
</div>
</div>
        <div class="sidebar-common-sidebar hidden"><div class="sidebar-author">
  <img data-src="/avatar/avatar.webp" data-sizes="auto" alt="Yyjccc" class="lazyload">
  <div class="sidebar-author-name">Yyjccc</div>
  <div class="sidebar-description"></div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>文章</div>
    <div class="sidebar-state-number">47</div>
  </div>
  <div class="sidebar-state-category">
    <div>分类</div>
    <div class="sidebar-state-number">7</div>
  </div>
  <div class="sidebar-state-tag">
    <div>标签</div>
    <div class="sidebar-state-number">72</div>
  </div>
</div>
<div class="sidebar-social">
  
    <div class="icon-github sidebar-social-icon">
      <a href=https://github.com/Yyjccc itemprop="url" target="_blank" aria-label="github"></a>
    </div>
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/" aria-label="首页"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">首页</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/archives" aria-label="归档"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">归档</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/about" aria-label="关于"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">关于</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/friend" aria-label="友链"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">友链</div>
    </div>
  
</div>
</div>
      
      
        <div class="sidebar-btn-wrapper" style="position:static">
          <div class="sidebar-toc-btn current"></div>
          <div class="sidebar-common-btn"></div>
        </div>
      
    </div>
  </div>

  
</aside>

          
        </div>
        <footer id="footer">
  <div style="width: 100%; overflow: hidden">
    <div class="footer-line"></div>
  </div>
  <div id="footer-info">
    
    <div>
      <span class="icon-copyright"></span>
      2023-2025
      <span class="footer-info-sep"></span>
      Yyjccc
    </div>
    
      <div>
        基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
        Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
      </div>
    
    
      <div>
        <span class="icon-brush"></span>
        163.6k
        &nbsp;|&nbsp;
        <span class="icon-coffee"></span>
        11:09
      </div>
    
    
      <div>
        <span class="icon-eye"></span>
        <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
        &nbsp;|&nbsp;
        <span class="icon-user"></span>
        <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
      </div>
    
  </div>
</footer>

        <div class="sidebar-top">
          <img src="/images/taichi.png" height="50" width="50" alt="backtop" />
          <div class="arrow-up"></div>
        </div>
        <div id="mask"></div>
      </div>
      <nav id="mobile-nav">
  <div class="sidebar-wrap">
    
      <div class="sidebar-toc-sidebar"><div class="sidebar-toc">
  <h3 class="toc-title">文章目录</h3>
  <div class="sidebar-toc-wrapper toc-div-class" >
      
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear-Regression%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">线性回归（Linear Regression）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">线性回归方程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.2.2.</span> <span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E6%AE%8A%E8%A7%A3"><span class="toc-number">1.2.3.</span> <span class="toc-text">特殊解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88BGD%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">批量梯度下降（BGD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88SGD%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">随机梯度下降法（SGD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88MBGD%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">小批量梯度下降法（MBGD）</span></a></li></ol></li></ol></li></ol>
      
  </div>
</div>
</div>
      <div class="sidebar-common-sidebar hidden"><div class="sidebar-author">
  <img data-src="/avatar/avatar.webp" data-sizes="auto" alt="Yyjccc" class="lazyload">
  <div class="sidebar-author-name">Yyjccc</div>
  <div class="sidebar-description"></div>
</div>
<div class="sidebar-state">
  <div class="sidebar-state-article">
    <div>文章</div>
    <div class="sidebar-state-number">47</div>
  </div>
  <div class="sidebar-state-category">
    <div>分类</div>
    <div class="sidebar-state-number">7</div>
  </div>
  <div class="sidebar-state-tag">
    <div>标签</div>
    <div class="sidebar-state-number">72</div>
  </div>
</div>
<div class="sidebar-social">
  
    <div class="icon-github sidebar-social-icon">
      <a href=https://github.com/Yyjccc itemprop="url" target="_blank" aria-label="github"></a>
    </div>
  
</div>
<div class="sidebar-menu">
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/" aria-label="首页"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">首页</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/archives" aria-label="归档"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">归档</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/about" aria-label="关于"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">关于</div>
    </div>
  
    <div class="sidebar-menu-link-wrap">
      <a class="sidebar-menu-link-dummy" href="/friend" aria-label="友链"></a>
      <div class="sidebar-menu-icon icon-taichi"></div>
      <div class="sidebar-menu-link">友链</div>
    </div>
  
</div>
</div>
    
  </div>
  
    <div class="sidebar-btn-wrapper">
      <div class="sidebar-toc-btn current"></div>
      <div class="sidebar-common-btn"></div>
    </div>
  
</nav>

    </div>
    
      <div class="site-search">
        <div class="reimu-popup popup">
          <div class="reimu-search">
            <div class="reimu-search-input-icon"></div>
            <div class="reimu-search-input" id="reimu-search-input"></div>
            <div class="popup-btn-close"></div>
          </div>
          <div class="reimu-results">
            <div id="reimu-stats"></div>
            <div id="reimu-hits"></div>
            <div id="reimu-pagination" class="reimu-pagination"></div>
          </div>
          <img class="reimu-bg" src="/images/reimu.png"/>
        </div>
      </div>
    
    
<script src="https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js"></script>



<script src="/js/script.js"></script>



  
<script src="/js/aos.js"></script>

  <script>
    var aosInit = () => {
      AOS.init({
        duration: 1000,
        easing: "ease",
        once: true,
        offset: 50,
      });
    };
    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', aosInit);
    } else {
      aosInit();
    }
  </script>



<script src="/js/pjax_script.js" data-pjax></script>


<script type="module" data-pjax>
  import PhotoSwipeLightbox from "https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe-lightbox.esm.min.js";
  
  const pswp = () => {
    if (_$$('.article-entry a.article-gallery-item').length > 0) {
      new PhotoSwipeLightbox({
        gallery: '.article-entry',
        children: 'a.article-gallery-item',
        pswpModule: () => import("https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.esm.min.js")
      }).init();
    }
    if(_$$('.article-gallery a.article-gallery-item').length > 0) {
      new PhotoSwipeLightbox({
        gallery: '.article-gallery',
        children: 'a.article-gallery-item',
        pswpModule: () => import("https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.esm.min.js")
      }).init();
    }
    window.lightboxStatus = 'done';
    window.removeEventListener('lightbox:ready', pswp);
  }
  if(window.lightboxStatus === 'ready') {
    pswp()
  } else {
    window.addEventListener('lightbox:ready', pswp);
  }
</script>






  
<script src="https://npm.webcache.cn/gitalk/dist/gitalk.min.js" data-pjax></script>

  
  <script data-pjax>
    var gitalkId = location.pathname;
    var gitalk = new Gitalk({
      clientID: '0efb4a442bc09d1ba17e',
      clientSecret: 'd26f520fef841de4f4388029a0d5c0f4730357cf',
      repo: 'https://github.com/Yyjccc/yyjccc.github.io.git',
      owner: 'Yyjccc',
      admin: "Yyjccc",
      id: gitalkId, // Ensure uniqueness and length less than 50
      distractionFreeMode: false // Facebook-like distraction free mode
    })
    if (document.getElementById('comments')) {
      gitalk.render('comments')
    }
  </script>







  
<script src="/js/generator_search.js" defer></script>






  
<script src="https://npm.webcache.cn/mouse-firework@0.0.4/dist/index.umd.js"></script>

  <script>
    firework(JSON.parse('{"excludeElements":["a","button"],"particles":[{"shape":"circle","move":["emit"],"easing":"easeOutExpo","colors":["#ff5252","#ff7c7c","#ffafaf","#ffd0d0"],"number":20,"duration":[1200,1800],"shapeOptions":{"radius":[16,32],"alpha":[0.3,0.5]}},{"shape":"circle","move":["diffuse"],"easing":"easeOutExpo","colors":["#ff0000"],"number":1,"duration":[1200,1800],"shapeOptions":{"radius":20,"alpha":[0.2,0.5],"lineWidth":6}}]}'))
  </script>





  <script>
    function initLive2d() {
      live2d.init("https://fastly.jsdelivr.net/gh/D-Sketon/plugin-live2d/", {themeTipsPath: ""});
    }
  </script>
  
<script src="https://fastly.jsdelivr.net/gh/D-Sketon/plugin-live2d/js/live2d-autoload.js" onload="initLive2d &amp;&amp; initLive2d()" async></script>




  
<script src="https://npm.webcache.cn/quicklink@2.3.0/dist/quicklink.umd.js"></script>

  <script data-pjax>
    quicklink.listen({
      timeout: 3000,
      priority: true,
      ignores: []
    });
  </script>


<div id="lazy-script">
  <div>
    
  </div>
</div>


  <script>
    console.log(String.raw`%c 
 ______     ______     __     __    __     __  __    
/\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
\ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
 \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
  \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                  
`,'color: #ff5252;')
    console.log('%c Theme.Reimu v' + '0.3.0-alpha.1' + ' %c https://github.com/D-Sketon/hexo-theme-reimu ', 'color: white; background: #ff5252; padding:5px 0;', 'padding:4px;border:1px solid #ff5252;')
  </script>
  

<script data-pjax>
  var updateTime = _$('#post-update-time')?.innerHTML;

  if (updateTime) {
    const update = new Date(updateTime);
    const now = new Date();
    const diff = now - update;
    const days = diff / 86400000;
    const { daysAgo, message: template } = window.outdate;
    if (days >= daysAgo) {
      const message = template.replace(/{time}/, updateTime);
      const blockquote = _$('#outdate-blockquote');
      if (blockquote) {
        blockquote.querySelector('p').innerText = message;
        blockquote.style.display = 'block';
      }
    }
  }
</script>


  
<script src="https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js" async></script>




<script>
  if ('serviceWorker' in navigator) {
    navigator.serviceWorker.getRegistrations().then((registrations) => {
      for (let registration of registrations) {
        registration.unregister();
      }
    });
  }
</script>

  <!-- hexo injector body_end start -->
<script src="/js/insert_highlight.js" data-pjax></script>
<!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true,"react":{"opacity":0.7}}});</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body>
  </html>

